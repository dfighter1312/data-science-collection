{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e09c83-107c-46c5-bae8-aefeabf30f10",
   "metadata": {},
   "source": [
    "# LangChain Quickstart Follow-up\n",
    "\n",
    "This notebook is a follow up on LangChain Quickstart Guide, which is available in its [documentation](https://python.langchain.com/en/latest/getting_started/getting_started.html). It includes additional notes for further understanding.\n",
    "\n",
    "## 1. Get predictions from a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8c90f0-3706-4df4-a1f2-cad9699abe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv('../.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce791d91-b7e7-46ca-b523-ef29b475bf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Found'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if environment variables have been loaded\n",
    "import os\n",
    "\n",
    "os.environ['TEST_VAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c94d2b-af53-43a5-b6dd-1027712af39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Two plus two times nine equals 36.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Temperature governs the randomness and thus the creativity of the responses.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "text = \"What is two plus two times nine equals?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a41bc4-9d95-4982-9485-d168b83fbbae",
   "metadata": {},
   "source": [
    "## 2. Manage Prompts for LLMs\n",
    "\n",
    "This is kinda like defining and replacing a variable in a text, for example:\n",
    "\n",
    "```python\n",
    "thing = \"line chart\"\n",
    "print(f'What is a {thing}')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89eaeee7-270c-4000-8220-e78704166708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain A/B test in data science.\n",
      "Explain Random Forest in data science.\n",
      "Explain Large Language Model in data science.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "data_science_prompt = PromptTemplate(\n",
    "    input_variables=[\"thing\"],\n",
    "    template=\"Explain {thing} in data science.\"\n",
    ")\n",
    "\n",
    "print(data_science_prompt.format(thing=\"A/B test\"))\n",
    "print(data_science_prompt.format(thing=\"Random Forest\"))\n",
    "print(data_science_prompt.format(thing=\"Large Language Model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ec6282-c89f-4a1a-b1ff-a4d7bf276f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain A/B test in data science.\n",
      "\n",
      "\n",
      "A/B testing is a method of data science used to compare two different versions of a product or feature in order to determine which one is more effective. In an A/B test, a group of users is randomly divided into two groups, each of which is given a different version of the product or feature to use. The success of one version is then measured against the other in order to determine which is more effective. A/B tests are used to optimize a product for maximum impact, and are widely used in online marketing, website design, product development, and more.\n",
      "--------------------------------------------------\n",
      "Explain Random Forest in data science.\n",
      "\n",
      "\n",
      "Random Forest is an ensemble learning method used for classification and regression that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random Forest leverages the ‘wisdom of the crowd’, which essentially means that it is less prone to overfitting to individual data points. It increases accuracy by reducing the variance caused by individual trees. Random Forest also provides a way to measure the relative importance of each feature in the prediction.\n",
      "--------------------------------------------------\n",
      "Explain Large Language Model in data science.\n",
      "\n",
      "\n",
      "A large language model in data science is a machine learning model trained on a large corpus of text data for the purpose of predicting the next word in a sequence or generating new text. The model works by encoding the text data into a numerical representation using an algorithm such as word embedding, and then using the numerical representation as input to a deep neural network. This deep neural network is then trained on the text data to learn patterns in the data and to produce predictions for new data. The predictions from the model can be used in various natural language processing tasks, such as sentiment analysis, text generation, and language translation.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for thing in [\"A/B test\", \"Random Forest\", \"Large Language Model\"]:\n",
    "    formatted_prompt = data_science_prompt.format(thing=thing)\n",
    "    print(formatted_prompt)\n",
    "    print(llm(formatted_prompt))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9a175-648c-4601-aece-7b5ca8ae9bfa",
   "metadata": {},
   "source": [
    "## 3. Chains: Combine LLMs and prompts in multi-step workflows\n",
    "\n",
    "We could have shorten the code in section 2 by using chains as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92eab31f-c657-481b-a630-586217589ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=data_science_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60f043cb-7966-431b-952f-20e0a5e80a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A/B test\n",
      "\n",
      "\n",
      "A/B testing, also known as split testing, is a statistical method that allows data scientists to compare two versions of a web page, application, or product with the goal of determining which one performs better. It is used to determine which version will have the greatest impact on user behavior and provide the most benefits. In an A/B test, a single variable (the \"A\" or the \"B\") is tested to determine which performs better across a variety of metrics. For example, a company may test two versions of a landing page to see which one produces more conversions.\n",
      "--------------------------------------------------\n",
      "Random Forest\n",
      "\n",
      "\n",
      "Random Forest is an ensemble method for classifying and predicting data in machine learning. It creates a group of decision trees (an ensemble) from a randomly selected subset of training data. Each tree in the ensemble is trained on a different subset of features and then makes predictions independently. The final prediction is determined by a majority vote or average of all of the individual trees’ predictions. Random forests are known for their stability, robustness, and accuracy with data that contains many features and interactions between them. They also require less data preparation compared to other methods, and are generally faster to train.\n",
      "--------------------------------------------------\n",
      "Large Language Model\n",
      "\n",
      "\n",
      "A large language model in data science is a type of artificial intelligence (AI) system that is trained to generate text or language-based data by learning from a large training corpus. This type of model is useful for tasks like predicting the next word in a sentence, understanding what a user might be asking, or generating new text or ideas from an input source. These models are typically built using deep learning algorithms and are trained on large datasets. Large language models can also be used for applications such as text summarization, machine translation, sentiment analysis, and recognizing entities in text.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for thing in [\"A/B test\", \"Random Forest\", \"Large Language Model\"]:\n",
    "    print(thing)\n",
    "    print(chain.run(thing))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a52afc-989a-4204-96b9-c79c657f15f0",
   "metadata": {},
   "source": [
    "## 4. Dynamically Call Chains Based on User Input\n",
    "\n",
    "Contexts for agents:\n",
    "- [Tool](https://python.langchain.com/en/latest/modules/agents/tools/getting_started.html): A function that performs a specific duty, note that some require an attached API key. Some available tools include:\n",
    "  - `python_repl`: Python shell for executing python commands.\n",
    "  - `serpapi`: Search engine, useful for when you need to answer questions about current events.\n",
    "  - `wolfram-alpha`: Wolfram alpha search engine for mathematics, science, technology, cultural, social questions.\n",
    "  - `requests`: Python requests module, get a specific URL as input, and output will be text on that page.\n",
    "  - `terminal`: Execute commands in a terminal.\n",
    "  - `llm-math`: Useful when need to answer questions about math.\n",
    "  - `wikipedia`: Search on Wikipedia.\n",
    "- LLM: The language model powering the agent.\n",
    "- Agent: The agent to use. It can be a built-in or customized agent. This notebook include an example of a built-in agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db2749a5-5554-46de-9cae-47389bb59b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Load tools\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Initialize the agent\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35ceca92-1fda-4eb8-8092-51a9f40dbc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the temperature in Hanoi yesterday\n",
      "Action: Search\n",
      "Action Input: \"maximum temperature in Hanoi yesterday\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHanoi Temperature Yesterday. Maximum temperature yesterday: 102 °F (at 2:30 pm) Minimum temperature yesterday: 79 °F (at 1:30 am) Average temperature ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to convert the temperature from Fahrenheit to Celsius\n",
      "Action: Calculator\n",
      "Action Input: 102 °F to Celsius\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 38.888888888888886\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The maximum temperature in Hanoi yesterday in Celsius was 38.888888888888886 degrees, which is greater than 30 degrees.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The maximum temperature in Hanoi yesterday in Celsius was 38.888888888888886 degrees, which is greater than 30 degrees.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test a query\n",
    "query = \"What is the maximum temperature in Hanoi yesterday in Celsius? Is it greater than 30 degree.\"\n",
    "agent.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fe2cf-e05b-41e2-b06e-c8252d55a4a3",
   "metadata": {},
   "source": [
    "The Agent type decides which actions to taken and in what order. Options can be found [here](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html).\n",
    "- `zero-shot-react-description`: Use ReAct framework to determine which tool to use based solely on the tool's description.\n",
    "- `react-docstore`: Interact with a docstore, using 2 tools, a `Search` tool and a `Lookup` tool.\n",
    "- `self-ask-with-search`: Lookup factual answers to questions.\n",
    "- `conversational-react-description`: Designed to make the agent helpful and conversational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d00d7f49-6356-493f-8315-f38e67edb94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZERO_SHOT_REACT_DESCRIPTION',\n",
       " 'REACT_DOCSTORE',\n",
       " 'SELF_ASK_WITH_SEARCH',\n",
       " 'CONVERSATIONAL_REACT_DESCRIPTION',\n",
       " 'CHAT_ZERO_SHOT_REACT_DESCRIPTION',\n",
       " 'CHAT_CONVERSATIONAL_REACT_DESCRIPTION',\n",
       " 'STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All posible AgentType\n",
    "AgentType._member_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b929c94f-7893-4d5f-8f4d-026fa2d53bc5",
   "metadata": {},
   "source": [
    "## 5. Memory: Add State to Chains and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8d573ee-33f4-449d-baf3-89487f3afe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c50763a6-8e18-454d-9c4a-c10517696699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi colleague\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. I'm an AI created to help you with your work. What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"Hi colleague\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6589751-1db9-4828-a973-2dff82a488ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi colleague\n",
      "AI:  Hi there! It's nice to meet you. I'm an AI created to help you with your work. What can I do for you today?\n",
      "Human: I am having a problem with my project. Can you help me?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Absolutely! What kind of problem are you having?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"I am having a problem with my project. Can you help me?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f00b3c9-e1e1-4ab8-aaae-e1bf9454e852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi colleague\n",
      "AI:  Hi there! It's nice to meet you. I'm an AI created to help you with your work. What can I do for you today?\n",
      "Human: I am having a problem with my project. Can you help me?\n",
      "AI:  Absolutely! What kind of problem are you having?\n",
      "Human: What is LangChain and possible usecases to deploy it in production?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " LangChain is a blockchain-based platform that enables developers to create decentralized applications (dApps) that are language-agnostic. It allows developers to create applications that can be used across multiple languages, making it easier to develop and deploy applications in production. Possible use cases for LangChain include creating multi-language dApps, creating secure and reliable data storage solutions, and creating secure and reliable payment solutions.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"What is LangChain and possible usecases to deploy it in production?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cca39573-59e6-4a78-b456-db7f0afb8a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi colleague\n",
      "AI:  Hi there! It's nice to meet you. I'm an AI created to help you with your work. What can I do for you today?\n",
      "Human: I am having a problem with my project. Can you help me?\n",
      "AI:  Absolutely! What kind of problem are you having?\n",
      "Human: What is LangChain and possible usecases to deploy it in production?\n",
      "AI:  LangChain is a blockchain-based platform that enables developers to create decentralized applications (dApps) that are language-agnostic. It allows developers to create applications that can be used across multiple languages, making it easier to develop and deploy applications in production. Possible use cases for LangChain include creating multi-language dApps, creating secure and reliable data storage solutions, and creating secure and reliable payment solutions.\n",
      "Human: It seems weird, I don't see it as a blockchain-based platform, I am using it to chat to you\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Ah, I see. LangChain is actually a platform that enables developers to create decentralized applications (dApps) that are language-agnostic. It is not specifically designed for chatting, but it can be used for that purpose. For example, you could use LangChain to create a secure and reliable chat application that can be used across multiple languages.\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"It seems weird, I don't see it as a blockchain-based platform, I am using it to chat to you\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a75ad2-be29-44c9-bb77-479055f33793",
   "metadata": {},
   "source": [
    "I believe that LangChain has been recently released, therefore LLM does not know about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39865de4-830a-4a78-8cf2-54592a0dcda7",
   "metadata": {},
   "source": [
    "## 6. Chat Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cd3638d-4d1d-42e1-81dd-933987526d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Initialize the LLM Chat Model\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Initialize the templates\n",
    "template = \"You are a senior data scientist who are trying to explain a concept in {language} for a person at {level} level\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "417d3a45-f65c-41d7-a73a-72714ae9e184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One hot encoding là một kỹ thuật được sử dụng trong machine learning để biến đổi các giá trị của một biến thành các vector nhị phân. Ví dụ, nếu chúng ta có một biến \"màu sắc\" với các giá trị \"đỏ\", \"xanh lá cây\" và \"xanh dương\", chúng ta có thể sử dụng one hot encoding để biến đổi các giá trị này thành các vector nhị phân. Vector cho giá trị \"đỏ\" sẽ có giá trị 1 tại vị trí đầu tiên và giá trị 0 tại các vị trí còn lại, vector cho giá trị \"xanh lá cây\" sẽ có giá trị 1 tại vị trí thứ hai và giá trị 0 tại các vị trí còn lại, và vector cho giá trị \"xanh dương\" sẽ có giá trị 1 tại vị trí thứ ba và giá trị 0 tại các vị trí còn lại. Kỹ thuật này giúp cho các giá trị của biến có thể được sử dụng trong các mô hình machine learning một cách dễ dàng và hiệu quả.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the chain\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(\n",
    "    language=\"Vietnamese\",\n",
    "    level=\"5-year-old\",\n",
    "    text=\"What is one hot encoding?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a9ead-c465-49e5-a5fd-e461b5cf2d69",
   "metadata": {},
   "source": [
    "## 7. Agents with Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2767d272-e2ab-4a97-a1a0-303bfcf79ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mLooks like Olivia Wilde and Jason Sudeikis are starting 2023 on good terms. Amid their highly publicized custody battle – and the actress' ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to use a calculator to raise Jason Sudeikis' age to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"47^0.23\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.4242784855673896\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know that Jason Sudeikis' current age raised to the 0.23 power is 2.4242784855673896.\n",
      "Final Answer: Jason Sudeikis' current age raised to the 0.23 power is 2.4242784855673896.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Jason Sudeikis' current age raised to the 0.23 power is 2.4242784855673896.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40a971-c38e-452e-b66f-1cba09702a78",
   "metadata": {},
   "source": [
    "## 8. Memory on Chat Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0aadf95-549e-4da9-aadc-a8d0e9fffc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "That sounds like fun! I'm happy to chat with you. What would you like to talk about?\n",
      "Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations with people like you! Is there anything specific you'd like to know about me?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "print(conversation.predict(input=\"Hi there!\"))\n",
    "print(conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\"))\n",
    "print(conversation.predict(input=\"Tell me about yourself.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
