{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624a6bb3-2246-4ca2-8e7c-5a8a1a2137d5",
   "metadata": {},
   "source": [
    "# Build apps with language models\n",
    "\n",
    "This is the first follow-up notebook from the list of tutorial.\n",
    "\n",
    "Link: https://www.youtube.com/watch?v=LbT1yp6quS8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52c00aef-e3d7-4067-89ae-cb90e6a037c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv('../.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b37b3a7-e31e-4178-a9d7-c91741bd356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n",
      "/Users/mufin/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How old sont vous?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import model from Hugging Face Hub\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-small\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_length\": 64\n",
    "    }\n",
    ")\n",
    "\n",
    "llm(\"Translate English to French: How old are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a817c6a5-bd74-412b-bded-607741fb8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd266643-e1d7-4f82-a70b-48378bcc0f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: Can you hear me?\\n\\nLet us think step by step\\n\\nAnswer:'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Let us think step by step\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt.format(question=\"Can you hear me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ea2c86-a228-4d83-a8aa-1506ddc93983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nStep 1: Are you speaking out loud?\\n\\nStep 2: Are you using a microphone or other audio device?\\n\\nStep 3: Is the audio device connected to a speaker or other audio output device?\\n\\nStep 4: Is the speaker or other audio output device turned on and working properly?\\n\\nIf the answer to all of these questions is yes, then the answer to your question is yes, you can be heard.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(question=\"Can you hear me?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8623818b-7448-4f56-9b91-f39263782c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It depends on what your question is. If it is something that I can answer, then yes, I can answer your question. If it is something that I do not know the answer to, then I cannot answer your question.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain\n",
    "from langchain import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"Can you answer my question?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d83c324-b55e-420a-979b-07e2f3330aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the year the film was released and then use the calculator to calculate the power.\n",
      "Action: Wikipedia\n",
      "Action Input: Departed with Leonardo Dicarpio\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should try searching for the movie title\n",
      "Action: Wikipedia\n",
      "Action Input: The Departed\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mufin/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/mufin/opt/anaconda3/envs/langchain/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: The Departed\n",
      "Summary: The Departed is a 2006 American epic crime thriller film directed by Martin Scorsese and written by William Monahan. It is both a remake of the 2002 Hong Kong film Infernal Affairs and also loosely based on the real-life Boston Winter Hill Gang; the character Colin Sullivan is based on the corrupt FBI agent John Connolly, while the character Frank Costello is based on Irish-American gangster and crime boss Whitey Bulger. The film stars Leonardo DiCaprio, Matt Damon, Jack Nicholson, and Mark Wahlberg, with Martin Sheen, Ray Winstone, Vera Farmiga, Alec Baldwin, Anthony Anderson and James Badge Dale in supporting roles.\n",
      "The film takes place in Boston and the surrounding metro area, primarily in the city’s South End neighborhood. Irish Mob boss Frank Costello (Nicholson) plants Colin Sullivan (Damon) as a spy within the Massachusetts State Police; simultaneously, the police assign undercover state trooper Billy Costigan (DiCaprio) to infiltrate Costello's mob crew. When both sides realize the situation, Sullivan and Costigan each attempt to discover the other's identity before they are found out.\n",
      "The Departed was a critical and commercial success, receiving acclaim for its direction, performances (particularly of DiCaprio, Nicholson, and Wahlberg), screenplay, and editing, with minor criticism for its plot holes. It won several accolades, including four Oscars at the 79th Academy Awards: for Best Picture, Best Director, Best Adapted Screenplay, and Best Film Editing. It became Scorsese's first and, to date, only personal Oscar win; Wahlberg was also nominated for Best Supporting Actor. The film also received six nominations at the 64th Golden Globe Awards, six nominations at the 60th British Academy Film Awards, and two nominations at the 13th Screen Actors Guild Awards. DiCaprio was nominated for Golden Globe Award for Best Actor – Motion Picture Drama (also nominated that year in the same category for Blood Diamond), BAFTA Award for Best Actor in a Leading Role and Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Supporting Role for his performance.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the year the film was released\n",
      "Action: Calculator\n",
      "Action Input: 2006^0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 26.30281917656938\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The film Departed with Leonardo Dicarpio was released in 2006 and this year raised to the 0.43 power is 26.30281917656938.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The film Departed with Leonardo Dicarpio was released in 2006 and this year raised to the 0.43 power is 26.30281917656938.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agents and Tools\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "agent.run('In what year was the film Departed with Leopnardo Dicarpio released? What is this year raised to the 0.43 power?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e76b28-a99b-4023-80aa-b2690bee75c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. My name is AI. What's your name?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory\n",
    "from langchain import ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbb73df-5fe9-4672-b31f-b2a3c2156614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: Can we talk about AI?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sure! What would you like to know about AI?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can we talk about AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b986f3ab-7bed-4a9d-8437-0d65383ada9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: Can we talk about AI?\n",
      "AI:  Sure! What would you like to know about AI?\n",
      "Human: I'm interested in how you were trained?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I was trained using a combination of supervised and unsupervised learning techniques. Supervised learning involves providing the AI with labeled data sets that it can use to learn from. Unsupervised learning involves giving the AI unlabeled data sets and allowing it to find patterns and correlations in the data.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm interested in how you were trained?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2ba5a69-aa2a-4005-b53f-a558bb9a320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"big.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1b9b30f-8dae-4f31-aa42-ed8f2263a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 3932, which is longer than the specified 3000\n",
      "Created a chunk of size 5781, which is longer than the specified 3000\n",
      "Created a chunk of size 3248, which is longer than the specified 3000\n",
      "Created a chunk of size 4125, which is longer than the specified 3000\n",
      "Created a chunk of size 3541, which is longer than the specified 3000\n",
      "Created a chunk of size 3054, which is longer than the specified 3000\n",
      "Created a chunk of size 3526, which is longer than the specified 3000\n",
      "Created a chunk of size 4387, which is longer than the specified 3000\n",
      "Created a chunk of size 3166, which is longer than the specified 3000\n",
      "Created a chunk of size 3322, which is longer than the specified 3000\n",
      "Created a chunk of size 3592, which is longer than the specified 3000\n",
      "Created a chunk of size 3376, which is longer than the specified 3000\n",
      "Created a chunk of size 3094, which is longer than the specified 3000\n",
      "Created a chunk of size 54862, which is longer than the specified 3000\n"
     ]
    }
   ],
   "source": [
    "# Text Splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c6b151-eabc-497b-b590-06169dd4396b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)a8e1d/.gitattributes: 100%|████████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 1.45MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|█████████████████████████████████████████████████████████████| 190/190 [00:00<00:00, 255kB/s]\n",
      "Downloading (…)b20bca8e1d/README.md: 100%|████████████████████████████████████████████████████████| 10.6k/10.6k [00:00<00:00, 2.98MB/s]\n",
      "Downloading (…)0bca8e1d/config.json: 100%|█████████████████████████████████████████████████████████████| 571/571 [00:00<00:00, 836kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|█████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 191kB/s]\n",
      "Downloading (…)e1d/data_config.json: 100%|█████████████████████████████████████████████████████████| 39.3k/39.3k [00:00<00:00, 197kB/s]\n",
      "Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████| 438M/438M [01:18<00:00, 5.61MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|███████████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 218kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█████████████████████████████████████████████████████████████| 239/239 [00:00<00:00, 432kB/s]\n",
      "Downloading (…)a8e1d/tokenizer.json: 100%|███████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 954kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████████████████████████████████████████████████████████████| 363/363 [00:00<00:00, 657kB/s]\n",
      "Downloading (…)8e1d/train_script.py: 100%|████████████████████████████████████████████████████████| 13.1k/13.1k [00:00<00:00, 3.04MB/s]\n",
      "Downloading (…)b20bca8e1d/vocab.txt: 100%|██████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 1.08MB/s]\n",
      "Downloading (…)bca8e1d/modules.json: 100%|█████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 652kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "398c1195-d81c-4fc9-9369-380ce6e1b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "query = \"Did Holmes return to the Baker Street by 3 o'clock?\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0e074f0-1ef7-4cb7-aa15-f1bc1a7d8b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I had been delayed at a case, and it was a little after half-past six when I found myself in Baker Street once more. As I approached the house I saw a tall man in a Scotch bonnet with a coat which was buttoned up to his chin waiting outside in the bright semicircle which was thrown from the fanlight. Just as I arrived the door was opened, and we were shown up together to Holmes\\' room.\\n\\n\"Mr. Henry Baker, I believe,\" said he, rising from his armchair and greeting his visitor with the easy air of geniality which he could so readily assume. \"Pray take this chair by the fire, Mr. Baker. It is a cold night, and I observe that your circulation is more adapted for summer than for winter. Ah, Watson, you have just come at the right time. Is that your hat, Mr. Baker?\"\\n\\n\"Yes, sir, that is undoubtedly my hat.\"\\n\\nHe was a large man with rounded shoulders, a massive head, and a broad, intelligent face, sloping down to a pointed beard of grizzled brown. A touch of red in nose and cheeks, with a slight tremor of his extended hand, recalled Holmes\\' surmise as to his habits. His rusty black frock-coat was buttoned right up in front, with the collar turned up, and his lank wrists protruded from his sleeves without a sign of cuff or shirt. He spoke in a slow staccato fashion, choosing his words with care, and gave the impression generally of a man of learning and letters who had had ill-usage at the hands of fortune.\\n\\n\"We have retained these things for some days,\" said Holmes, \"because we expected to see an advertisement from you giving your address. I am at a loss to know now why you did not advertise.\"\\n\\nOur visitor gave a rather shamefaced laugh. \"Shillings have not been so plentiful with me as they once were,\" he remarked. \"I had no doubt that the gang of roughs who assaulted me had carried off both my hat and the bird. I did not care to spend more money in a hopeless attempt at recovering them.\"\\n\\n\"Very naturally. By the way, about the bird, we were compelled to eat it.\"\\n\\n\"To eat it!\" Our visitor half rose from his chair in his excitement.\\n\\n\"Yes, it would have been of no use to anyone had we not done so. But I presume that this other goose upon the sideboard, which is about the same weight and perfectly fresh, will answer your purpose equally well?\"\\n\\n\"Oh, certainly, certainly,\" answered Mr. Baker with a sigh of relief.\\n\\n\"Of course, we still have the feathers, legs, crop, and so on of your own bird, so if you wish--\"\\n\\nThe man burst into a hearty laugh. \"They might be useful to me as relics of my adventure,\" said he, \"but beyond that I can hardly see what use the disjecta membra of my late acquaintance are going to be to me. No, sir, I think that, with your permission, I will confine my attentions to the excellent bird which I perceive upon the sideboard.\"\\n\\nSherlock Holmes glanced sharply across at me with a slight shrug of his shoulders.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30485b7a-9bcc-4c42-99ac-748a3a573d21",
   "metadata": {},
   "source": [
    "### Additional Practice - Analyzing a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35c945f7-d3d2-4221-8395-0afde2f28c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\"2206.04564\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14a28149-f8d8-4af8-a4b8-866bd87a759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitter\n",
    "from langchain.text_splitter import LatexTextSplitter\n",
    "\n",
    "text_splitter = LatexTextSplitter()\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5b81544-36e5-4324-b625-979449d3ec1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TwiBot-22: Towards Graph-Based\\nTwitter Bot Detection\\nShangbin Feng1,2∗ Zhaoxuan Tan1∗ Herun Wan1∗ Ningnan Wang1∗ Zilong Chen1,3∗ Binchi Zhang1,4∗\\nQinghua Zheng1† Wenqian Zhang1\\nZhenyu Lei1\\nShujie Yang1\\nXinshun Feng1\\nQingyue Zhang1\\nHongrui Wang1\\nYuhan Liu1\\nYuyang Bai1\\nHeng Wang1\\nZijian Cai1\\nYanbo Wang1\\nLijing Zheng1\\nZihan Ma1\\nJundong Li4\\nMinnan Luo1\\nXi’an Jiaotong University1, University of Washington2, Tsinghua University3, University of Virginia4\\ncontact: shangbin@cs.washington.edu\\nAbstract\\nTwitter bot detection has become an increasingly important task to combat\\nmisinformation, facilitate social media moderation, and preserve the integrity of\\nthe online discourse. State-of-the-art bot detection methods generally leverage the\\ngraph structure of the Twitter network, and they exhibit promising performance\\nwhen confronting novel Twitter bots that traditional methods fail to detect.\\nHowever, very few of the existing Twitter bot detection datasets are graph-based,\\nand even these few graph-based datasets suffer from limited dataset scale,\\nincomplete graph structure, as well as low annotation quality. In fact, the lack of\\na large-scale graph-based Twitter bot detection benchmark that addresses these\\nissues has seriously hindered the development and evaluation of novel graph-based\\nbot detection approaches. In this paper, we propose TwiBot-22, a comprehensive\\ngraph-based Twitter bot detection benchmark that presents the largest dataset\\nto date, provides diversiﬁed entities and relations on the Twitter network, and\\nhas considerably better annotation quality than existing datasets. In addition,\\nwe re-implement 35 representative Twitter bot detection baselines and evaluate\\nthem on 9 datasets, including TwiBot-22, to promote a fair comparison of model\\nperformance and a holistic understanding of research progress. To facilitate\\nfurther research, we consolidate all implemented codes and datasets into the\\nTwiBot-22 evaluation framework, where researchers could consistently evaluate\\nnew models and datasets. The TwiBot-22 Twitter bot detection benchmark and\\nevaluation framework are publicly available at https://twibot22.github.io/.\\n1\\nIntroduction\\nAutomated users on Twitter, also known as Twitter bots, have become a widely known and well-\\ndocumented phenomenon. Over the past decade, malicious Twitter bots were responsible for a wide\\nrange of problems such as online disinformation [Cui et al., 2020, Wang et al., 2020, Lu and Li,\\n2020], election interference [Howard et al., 2016, Bradshaw et al., 2017, Rossi et al., 2020, Ferrara,\\n2017], extremism campaign [Ferrara et al., 2016, Marcellino et al., 2020], and even the spread of\\nconspiracy theories [Ferrara, 2020, Ahmed et al., 2020, Anwar et al., 2021]. These societal challenges\\nhave called for automatic Twitter bot detection models to mitigate their negative inﬂuence.\\n∗ These authors contributed equally to this work.\\n† Corresponding author: Qinghua Zheng, School of Computer Science and Technology, Xi’an Jiaotong\\nUniversity. Email: qhzheng@xjtu.edu.cn; Minnan luo, School of Computer Science and Technology, Xi’an\\nJiaotong University. Email: minnluo@xjtu.edu.cn\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.\\narXiv:2206.04564v6  [cs.SI]  12 Feb 2023\\nFigure 1: Compared to real-world Twitter (below), existing Twitter bot detection datasets (above)\\nsuffer from (a) limited dataset scale, (b) incomplete graph structure, and (c) poor annotation quality.\\nExisting Twitter bot detection models are generally feature-based, where researchers propose to\\nextract numerical features from user information such as metadata [Yang et al., 2020, Lee et al.,\\n2011], user timeline [Mazza et al., 2019, Chavoshi et al., 2016], and follow relationships [Beskow\\nand Carley, 2020, Chu et al., 2012]. However, feature-based approaches are susceptible to adversarial\\nmanipulation, i.e., when bot operators try to avoid detection by tampering with these hand-crafted\\nfeatures [C'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97f3cb0c-f4dd-4764-90d0-91849a25225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04cc7109-4892-4106-bac2-2f8301eeeb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: ../chroma\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    collection_name=\"deformable-graph-transformer\",\n",
    "    persist_directory=\"../chroma\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c995c16-d8e4-4bf0-bc6f-aa7162d5b7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chroma collection deformable-graph-transformer contains fewer than 4 elements.\n",
      "Chroma collection deformable-graph-transformer contains fewer than 3 elements.\n",
      "Chroma collection deformable-graph-transformer contains fewer than 2 elements.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many users are there in the Twibot-22 dataset?\n",
      "Answer:  I don't know.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "prompt = \"How many users are there in the Twibot-22 dataset?\"\n",
    "response = chain({\n",
    "    \"question\": prompt,\n",
    "    \"chat_history\": \"\"\n",
    "})\n",
    "\n",
    "# Retrieve the answer\n",
    "answer = response[\"answer\"]\n",
    "source = response[\"source_documents\"]\n",
    "print(\"Question:\", prompt)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28651c-7566-43d5-bfd2-fc16645a29a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
