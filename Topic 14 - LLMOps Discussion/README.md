# Topic 14 - Backgrounds and Considerations for LLMOps

## What this repository do?

This is a special topic where I focus on the theoretical aspect rather than implementation. The topic is LLMOps, or in other word, Large Language Model Operations. The term may not be widely accepted since it is closely related to MLOps (Machine Learning Operations), with the only difference is that we are having a really complex model (except for proprietary models where we only have an access to the model via APIs, not the source code itself).

I have found several useful material so far, and will analyze it a bit, one by one, to see how the world discusses about Large Language Model, in the Report section. You can find them in the References section.

## References

### Theoretical Background on Transformers and GPTs

Understanding Flow: Attention Mechanism --> Transformer --> GPT (decoder of Transformer) --> GPT-1 --> GPT-2 (unsupervised learning) --> GPT-3 (providing context) --> Reinforcement Learning with Human Feedback --> InstructGPT.

In fact there is a report of GPT-4, but it is just a report of how the new one improves.

1. [Transformer] "Attention is all you need" (Paper, 2017) [[link]](https://arxiv.org/pdf/1706.03762.pdf).
2. [GPT-1] "Improving Language Understanding by Generative Pre-Training" (Paper, 2018) [[link]](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).
3. [GPT-2] "Language Models are Unsupervised Multitask Learners" (Paper, 2018) [[link]](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
4. [GPT-3] "Language Models are Few-Shot Learners" [[link]](https://arxiv.org/pdf/2005.14165.pdf).
5. [RLHF a] "Deep reinforcement learning from human preferences" (Paper, 2017) [[link]](https://arxiv.org/pdf/1706.03741.pdf).
6. [RLHF b] "Learning to summarize from human feedback" (Paper, 2020) [[link]](https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf).
7. [InstructGPT] "Training language models to follow instructions with human feedback" (Paper, 2022) [[link]](https://arxiv.org/pdf/2203.02155.pdf).

### Byte-Pair Encoding Tokenization

This is the tokenization method that large language models apply. In "naive" tokenization, every word is a token. For example, *"This is a sentence"* will return us 4 tokens (This, is, a, sentence). However, BPE (Byte-Pair Encoding) is a simple and robust form of data compression in which the most common pair of contiguous bytes of data in a sequence are replaced with a byte that does not occur within the sequence. A lookup table of the replacements is required to rebuild the original data (by Wikipedia).

This is essential for researchers to know how much they would be cost for every query in the fine-tuning process.

8. "Byte Pair Encoding Tokenization" (Video) [[link]](https://www.youtube.com/watch?v=HEikzVL-lZU).

### LLMOps discussions

Proprietary or Open-source? On-premise or in the cloud? Should some transferring techniques be used? How to test, evaluate and deploy?

9. "Building LLM applications for production" (Blog) [[link]](https://huyenchip.com/2023/04/11/llm-engineering.html).
10. "LLMOps (LLM Bootcamp)" (Video) [[link]](https://www.youtube.com/watch?v=Fquj2u7ay40).

## Motivation

This is a bit odd that a README file in my collection has the motivation part. But it is strong enough that I include it here.

So far, I took part in an interview for a team that tries to employ LLM for there testing framework. The project was 2-month old then and the only thing they figured out is that ChatGPT knew about there product (well I think it is indeed since there product was release in about 2018-2019). Their understanding on LLM (or even about Natural Language Processing) made me confuse about the expected outcome of this project, since to perfectly adapt LLM, both theoretical, technical and financial aspects should be taken into consideration.

*In theoretical aspect*, we should have learned how useful an answer generated by LLM can be, and what is the minimum length from a prompt that clearly defines the objective so that LLM can understand and give such an useful answer. We should know that LLM is a generative model, and in common sense, it just give out the token with the highest probability given the previous $n$ tokens. Indeed $n$, or context window, is not indefinite. Hallucination or toxicity are typical concerns from both the authors and community, so keep it in mind.

*In technical aspect*, which LLM should we choose, which data should we collect, and how to deploy it. Currently I do not have such a clear understanding about every LLM existing in the industry right now and the number of LLMs is increasing significantly.

*In financial aspect*, every token counts. For open-source model that we would like to "download" it, fine-tuning it, then deploy it, then how much cost should it be. If context window is limited, so is our budget.

Although I do not know that I will be part of a team that employs a LLM model or not, this should worth exploring for any other machine learning or deep learning architecture for any problem that artificial intelligence can be a great supporter.

## Report

To be updated.
