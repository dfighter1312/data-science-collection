{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab10cbd-a1d5-4fc8-acad-118f145741e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 11:09:59 WARN Utils: Your hostname, Mufins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.245.5 instead (on interface en0)\n",
      "23/04/14 11:09:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 11:09:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/14 11:09:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession from builder\n",
    "# If the sample data you work with is small, you can remove the `.config` call\n",
    "spark = SparkSession.builder.appName('Spark') \\\n",
    "            .config(\"spark.driver.memory\", \"10g\") \\\n",
    "            .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd82a7b-44d7-4cfe-ac32-0bddf8b5a31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- entities: struct (nullable = true)\n",
      " |    |-- description: struct (nullable = true)\n",
      " |    |    |-- cashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- tag: string (nullable = true)\n",
      " |    |    |-- hashtags: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- tag: string (nullable = true)\n",
      " |    |    |-- mentions: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- username: string (nullable = true)\n",
      " |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |-- url: struct (nullable = true)\n",
      " |    |    |-- urls: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- display_url: string (nullable = true)\n",
      " |    |    |    |    |-- end: long (nullable = true)\n",
      " |    |    |    |    |-- expanded_url: string (nullable = true)\n",
      " |    |    |    |    |-- start: long (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- pinned_tweet_id: long (nullable = true)\n",
      " |-- profile_image_url: string (nullable = true)\n",
      " |-- protected: boolean (nullable = true)\n",
      " |-- public_metrics: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- following_count: long (nullable = true)\n",
      " |    |-- listed_count: long (nullable = true)\n",
      " |    |-- tweet_count: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- withheld: struct (nullable = true)\n",
      " |    |-- country_codes: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df = spark.read.json('user.json')\n",
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cea97f0-38cf-4ad8-9fb3-85b6d33eb5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                  id|\n",
      "+--------------------+\n",
      "|u1217628182611927040|\n",
      "|         u2664730894|\n",
      "|u1266703520205549568|\n",
      "|u1089159225148882949|\n",
      "|           u36741729|\n",
      "|u1365527332627247104|\n",
      "|         u1679822588|\n",
      "|         u1519144464|\n",
      "|           u15211869|\n",
      "|u1309034737756000256|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_df.select(\"id\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a812a0f-b22f-4c25-9afd-32f0ffe0ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_filter_df = spark.read.csv('sample_user.csv', header=True)\n",
    "user_df = user_filter_df.join(user_df, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7a504c8-5997-497c-adc3-0273201ee831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------------------+--------------------+--------------------+--------------------+-------------------+--------------------+---------+--------------------+--------------------+---------------+--------+--------+\n",
      "|                  id|          created_at|                         description|            entities|            location|                name|    pinned_tweet_id|   profile_image_url|protected|      public_metrics|                 url|       username|verified|withheld|\n",
      "+--------------------+--------------------+------------------------------------+--------------------+--------------------+--------------------+-------------------+--------------------+---------+--------------------+--------------------+---------------+--------+--------+\n",
      "|u1052241356574330880|2018-10-16 16:54:...|                             podador|                null|Lloret de Vistale...|               Nadal|               null|https://abs.twimg...|    false|{149, 2061, 0, 7151}|                    |  Nadal97836660|   false|    null|\n",
      "|u1382624255108653056|2021-04-15 09:18:...|                Gadgets, things, ...|                null|     Los Angeles, CA|       Thingy things|               null|https://pbs.twimg...|    false|{85231, 28, 311, ...|                    |     nerds_feed|   false|    null|\n",
      "|         u1468355888|2013-05-29 22:05:...|                Scientist at the ...|{null, {[{chrim.c...|  Winnipeg, Manitoba|          Joe Gordon|               null|https://pbs.twimg...|    false|{1471, 1873, 13, ...|https://t.co/TxlI...|  josephwgordon|   false|    null|\n",
      "|          u310714574|2011-06-04 06:52:...|                CDO at @stvalora_...|{{null, null, [{2...|              Madrid|       Santiago Mota|               null|https://pbs.twimg...|    false|{2246, 4988, 156,...|http://t.co/2vHWd...|  mota_santiago|   false|    null|\n",
      "|          u125984618|2010-03-24 13:43:...|                In Math We Trust,...|{{null, [{26, 18,...|            Istanbul|   Onur YUKSEKTEPELI|               null|https://pbs.twimg...|    false|{9290, 8688, 160,...|https://t.co/9wUa...|  oyuksektepeli|   false|    null|\n",
      "|           u25493114|2009-03-20 12:04:...|                Profesor Asociado...|{{null, null, [{2...|Bogotá, D.C., Col...|Juan David Gutiérrez|1075167362842812416|https://pbs.twimg...|    false|{7998, 3552, 89, ...|https://t.co/K0Tx...|       JuanDGut|   false|    null|\n",
      "|          u285530973|2011-04-21 09:50:...|                Casgliadau ac ymc...|{{null, null, [{1...|             Cardiff|     Cardiff Curator|               null|https://pbs.twimg...|    false|{7354, 3916, 182,...|https://t.co/QDfC...| CardiffCurator|   false|    null|\n",
      "|         u2967455656|2015-01-08 07:54:...|                Learning Data Sci...|{null, {[{bio.lin...|                null|Tawanda Nyahuye👨‍💻|1438373520833122307|https://pbs.twimg...|    false|{30598, 7456, 184...|https://t.co/0jYy...|      towernter|   false|    null|\n",
      "| u949778836048277504|2018-01-06 23:04:...|                @PsychSBU postdoc...|{{null, null, [{9...|            Brooklyn|       Benjamin Katz|1379454170198081543|https://pbs.twimg...|    false|{3427, 1993, 17, ...|https://t.co/WU55...|      DrBenKatz|   false|    null|\n",
      "|          u145637907|2010-05-19 13:14:...|                Experts analyse a...|{{null, null, [{8...|              London|LSE British Politics|               null|https://pbs.twimg...|    false|{71481, 8526, 158...|http://t.co/IlA3w...|LSEpoliticsblog|    true|    null|\n",
      "|           u27922157|2009-03-31 17:50:...|                American Medical ...|{{null, [{112, 92...|Chicago/Washingto...|                 AMA|1362081041880670216|https://pbs.twimg...|    false|{744084, 6830, 71...|http://t.co/MEg0g...|AmerMedicalAssn|    true|    null|\n",
      "|           u11178672|2007-12-14 21:43:...|                Seeking #sustcomm...|{{null, [{17, 8, ...|Hudson Highlands ...|Andrew Revkin 🌎 ...|               null|https://pbs.twimg...|    false|{89645, 10238, 42...|https://t.co/RTsE...|         Revkin|    true|    null|\n",
      "|           u16869351|2008-10-20 16:00:...|                https://t.co/aENq...|{{null, null, nul...|             Chicago|            TeachHUB|               null|https://pbs.twimg...|    false|{101175, 12091, 1...|http://t.co/q3bGF...|       TeachHub|   false|    null|\n",
      "|          u251951656|2011-02-14 04:58:...|                https://t.co/xISl...|{{null, [{47, 42,...|      Chennai, India|     EquityBulls.com|               null|https://pbs.twimg...|    false|{24404, 19, 310, ...|http://t.co/2raNR...|    equitybulls|   false|    null|\n",
      "|u1403349174893096965|2021-06-11 13:51:...|                                    |                null|                null|             OTY GÜN|               null|https://abs.twimg...|    false|   {24, 85, 0, 2512}|                    |  O_Bir_Kuldur_|   false|    null|\n",
      "|          u260288732|2011-03-03 15:48:...|                Showcasing Africa...|{null, {[{globala...|              Africa|Globalafricaawarenes|               null|https://pbs.twimg...|    false|   {93, 74, 3, 1339}|http://t.co/HWxeD...| GloAfricaAware|   false|    null|\n",
      "|         u3288441596|2015-07-23 05:32:...|アズ ノゥ アズの公式アカウントです。|                null|                null|      アズ ノゥ アズ|               null|https://pbs.twimg...|    false| {1059, 51, 11, 160}|                    |   asknowasblog|   false|    null|\n",
      "| u983979487225315328|2018-04-11 08:06:...|                Identifying and p...|{{null, null, [{1...|                null|Partnership for R...|1224725657969725440|https://pbs.twimg...|    false|{1449, 919, 22, 1...|https://t.co/733w...|     PROG_ocean|   false|    null|\n",
      "|          u553912317|2012-04-14 20:56:...|                ID physician @Emo...|{{null, null, [{1...|                null|Jess Howard-Anderson|               null|https://pbs.twimg...|    false| {754, 514, 17, 395}|                    |        JessH_A|   false|    null|\n",
      "|          u337664967|2011-07-18 11:42:...|                British Born Chin...|{{null, null, [{1...|                null|               Mambo|1465637030738731009|https://pbs.twimg...|    false|{1454, 2543, 74, ...|https://t.co/Fibd...|  mambo_doodles|   false|    null|\n",
      "+--------------------+--------------------+------------------------------------+--------------------+--------------------+--------------------+-------------------+--------------------+---------+--------------------+--------------------+---------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88047248-d131-442e-804e-e50c078e9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "user_df = user_df.withColumn('created_at_timestamp', F.to_date('created_at')) \\\n",
    "                .withColumn('age', F.datediff(F.current_date(), 'created_at'))\n",
    "user_df = user_df.withColumn('tweet_frequency', user_df['public_metrics.tweet_count'] / user_df['age']) \\\n",
    "                .withColumn('followers_growth_rate', user_df['public_metrics.followers_count'] / user_df['age']) \\\n",
    "                .withColumn('following_growth_rate', user_df['public_metrics.following_count'] / user_df['age']) \\\n",
    "                .withColumn('listed_growth_rate', user_df['public_metrics.listed_count'] / user_df['age']) \\\n",
    "                .withColumn('followers_following_ratio', user_df['public_metrics.followers_count'] / F.when(user_df['public_metrics.following_count'] == 0, 1).otherwise(user_df['public_metrics.following_count'])) \\\n",
    "                .withColumn('name_length', F.length('name')) \\\n",
    "                .withColumn('num_digits_in_name', F.length(F.regexp_extract('name', r'\\d+', 0))) \\\n",
    "                .withColumn('username_length', F.length('username')) \\\n",
    "                .withColumn('num_digits_in_username', F.length(F.regexp_extract('username', r'\\d+', 0)))\n",
    "                # .withColumn('description_length', F.length('description'))\n",
    "\n",
    "extracted_df = user_df.select([\n",
    "    'public_metrics.followers_count',\n",
    "    'public_metrics.following_count',\n",
    "    'public_metrics.listed_count',\n",
    "    'public_metrics.tweet_count',\n",
    "    'protected',\n",
    "    'verified',\n",
    "    'age',\n",
    "    'tweet_frequency',\n",
    "    'followers_growth_rate',\n",
    "    'following_growth_rate',\n",
    "    'followers_following_ratio',\n",
    "    'name_length',\n",
    "    'num_digits_in_name',\n",
    "    'username_length',\n",
    "    'num_digits_in_username'\n",
    "])\n",
    "# Cast each column to IntegerType\n",
    "for c in extracted_df.columns:\n",
    "    extracted_df = extracted_df.withColumn(c, F.col(c).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08eb8896-6b8b-42a8-83f6-252dba64d788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/14 11:12:31 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [1.27236530e+05 3.01896906e+03 4.69078113e+02 2.15224633e+04\n",
      " 7.73395205e-04 2.02629544e-01 3.69836350e+03 5.47563805e+00\n",
      " 2.80696056e+01 6.00928074e-01 1.00828770e+03 1.46674401e+01\n",
      " 4.40835267e-02 1.09474091e+01 3.63495746e-01]\n",
      "Standard Deviation:  [2.01244548e+06 1.26720187e+04 2.61769171e+03 4.76464280e+04\n",
      " 2.78099839e-02 4.02114246e-01 1.50535105e+03 1.40102926e+01\n",
      " 3.93984832e+02 3.44439228e+00 1.69221065e+04 7.28600801e+00\n",
      " 3.36719788e-01 2.71368847e+00 1.21941118e+00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "# Convert columns to a vector column\n",
    "assembler = VectorAssembler(inputCols=extracted_df.columns, outputCol=\"features\")\n",
    "vectorized_df = assembler.transform(extracted_df)\n",
    "\n",
    "# Standardize the vector column\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(vectorized_df)\n",
    "scaled_df = scalerModel.transform(vectorized_df)\n",
    "\n",
    "# Get mean and standard deviation\n",
    "mean = scalerModel.mean.toArray()\n",
    "std = scalerModel.std.toArray()\n",
    "\n",
    "# Print mean and standard deviation\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Standard Deviation: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14ce9472-a87b-4643-a92d-1e169fa765e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1293, 15])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "featuresArray = scaled_df.select(\"scaled_features\").rdd.flatMap(lambda x: x).collect()\n",
    "tfidfTensor = torch.tensor(featuresArray)\n",
    "tfidfTensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbfec06-1357-4f73-afc6-67cc8d11bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariedDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, torch_list):\n",
    "        self.torch_list = torch_list\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise ValueError(f\"Index exceeds length of the dataset of {len(self)}\")\n",
    "        return self.torch_list[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.torch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f90389f3-393a-439c-b303-389093ed99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = VariedDataset(tfidfTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "993b05c9-425a-4326-b681-4afc4f5b1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8)\n",
    "torch.save(dataloader, \"dataloaders/sample/user.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8604bb4a-56a6-495e-8ac5-79b1fe3ccab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean and standard deviation\n",
    "mean = scalerModel.mean.toArray()\n",
    "std = scalerModel.std.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f812538a-6872-47b6-a1dd-4df442299739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0212184a-81ac-4053-ae27-3fb2fa2b75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mean and std\n",
    "import numpy as np\n",
    "\n",
    "np.save(\"dataloaders/sample/user_mean.npy\", mean)\n",
    "np.save(\"dataloaders/sample/user_std.npy\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f26e75e0-1973-45af-a931-7dd8db0af69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                  id|label|\n",
      "+--------------------+-----+\n",
      "|u1217628182611927040|    0|\n",
      "|         u2664730894|    0|\n",
      "|u1266703520205549568|    0|\n",
      "|u1089159225148882949|    0|\n",
      "|           u36741729|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_df = spark.read.csv('label.csv', header=True)\n",
    "label_df = label_df.withColumn(\"label_bool\", F.col(\"label\") == \"bot\")\n",
    "label_df = label_df.withColumn(\"label_int\", F.col(\"label_bool\").cast(\"integer\"))\n",
    "label_df = label_df.select(F.col(\"id\"), F.col(\"label_int\").alias(\"label\"))\n",
    "label_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adea3660-7d8b-4e6c-9735-b1bcf78f5d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                  id|label|\n",
      "+--------------------+-----+\n",
      "|         u1468355888|    0|\n",
      "|           u25493114|    0|\n",
      "|         u2967455656|    0|\n",
      "|          u145637907|    0|\n",
      "|u1403349174893096965|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_df = user_filter_df.join(label_df, on=\"id\", how=\"left\")\n",
    "label_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e90a457e-e6b1-4f3b-a8f5-87da2ba2afda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1293"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.filter(label_df[\"label\"].isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b67920f8-d930-477d-8ee0-feadd9f9a8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1293"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelArray = label_df.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "len(labelArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12d69d6b-8eed-4ce8-a32e-7b513dc3c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTensor = torch.tensor(labelArray)\n",
    "dataset = VariedDataset(labelTensor)\n",
    "dataloader = DataLoader(dataset, batch_size=8)\n",
    "torch.save(dataloader, \"dataloaders/sample/user_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbf401-8826-42db-a677-a73dc13dd8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
