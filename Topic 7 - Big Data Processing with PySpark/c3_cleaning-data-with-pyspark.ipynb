{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d140c523-f6aa-4a5c-b6c0-1dbe6bf53b83",
   "metadata": {},
   "source": [
    "# Course 3: Cleaning Data with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33a69f3-75d5-443e-99f1-f175b81b1843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/31 10:15:07 WARN Utils: Your hostname, Mufins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.21 instead (on interface en0)\n",
      "23/03/31 10:15:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/31 10:15:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession from builder\n",
    "# If the sample data you work with is small, you can remove the `.config` call\n",
    "spark = SparkSession.builder.appName('Spark').config(\"spark.driver.memory\", \"15g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe28b409-cf3a-4aec-94ed-a46847cf989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e0c39-4564-420e-9895-dccc95495342",
   "metadata": {},
   "source": [
    "## 1. Introduction to Data Cleaning with Apache Spark\n",
    "\n",
    "### Data Cleaning with Spark\n",
    "\n",
    "Data cleaning is preparing raw data for use in data processing pipelines.\n",
    "\n",
    "Possible tasks in data cleaning includes:\n",
    "- Reformatting or replacing text\n",
    "- Perform calculations\n",
    "- Removing garbage or incomplete data\n",
    "\n",
    "Most data cleaning systems have 2 big problems: optimizing performance and organizing the flow of data. Typical programming languages wouldn't be able to process a massive amount of information in a timely manner. Spark lets you scale your data processing capacity as your requirements evolve.\n",
    "\n",
    "Using Spark transformations (refers to Course 2), we can create a PySpark DataFrame and continue processing afterwards.\n",
    "\n",
    "### Spark Schema\n",
    "\n",
    "A primary function of data cleaning is to verify all data is in the expected format. Spark provides a built-in ability to validate datasets with schemas.\n",
    "\n",
    "A schema defines and validates the number and types of columns for a given DataFrame. A schema can contain many different types of fields (int, float, date, str, array, struct). A defined schema allows Spark to filter out data that doesn't conform during read, ensuring expected correctness.\n",
    "\n",
    "Normally a data import will try to infer a schema on read - this requires reading the data twice. Defining a schema limits this to a single read operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5407c6fd-9812-40fd-b272-936c0c793e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('name', StringType(), True), StructField('age', IntegerType(), True), StructField('city', StringType(), True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Spark schema\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "peopleSchema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('city', StringType(), True)\n",
    "])\n",
    "peopleSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464a425-6120-4318-98b5-bb4627b41145",
   "metadata": {},
   "source": [
    "## 2. Immutability and lazy processing\n",
    "\n",
    "Python variables are mutable and flexible. Hence, they are potential for issues with concurrency, and likely add complexity.\n",
    "\n",
    "Unlike typical Python variables, Spark DataFrames are immutable. While not strictly required, immutability is often a component of functional programming. We won't go into everything that implies here, but understand that Spark is designed to use immutable objects. Practically, this means Spark DataFrames are defined once and are not modifiable after initialization. If the variable name is reused, the original data is removed and the variable name is reassigned to the new data. While this seems inefficient, it actually allows Spark to share data between all cluster components. It can do so without worry about concurrent data objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170830b7-a02f-4726-97b3-1acde702fc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|Date (MM/DD/YYYY)|Flight Number|Actual elapsed time (Minutes)|airport|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "|       01/01/2014|         0005|                          519|    hnl|\n",
      "|       01/01/2014|         0007|                          505|    ogg|\n",
      "|       01/01/2014|         0035|                          174|    slc|\n",
      "|       01/01/2014|         0043|                          153|    dtw|\n",
      "|       01/01/2014|         0052|                          137|    pit|\n",
      "|       01/01/2014|         0058|                          174|    san|\n",
      "|       01/01/2014|         0060|                          155|    mia|\n",
      "|       01/01/2014|         0064|                          185|    jfk|\n",
      "|       01/01/2014|         0090|                          126|    ord|\n",
      "|       01/01/2014|         0096|                           91|    stl|\n",
      "|       01/01/2014|         0099|                          182|    sna|\n",
      "|       01/01/2014|         0103|                          181|    ont|\n",
      "|       01/01/2014|         0109|                          127|    den|\n",
      "|       01/01/2014|         0122|                          222|    sfo|\n",
      "|       01/01/2014|         0123|                          510|    hnl|\n",
      "|       01/01/2014|         0129|                          114|    cos|\n",
      "|       01/01/2014|         0130|                          141|    dca|\n",
      "|       01/01/2014|         0131|                          167|    slc|\n",
      "|       01/01/2014|         0132|                           82|    stl|\n",
      "|       01/01/2014|         0140|                          146|    bwi|\n",
      "+-----------------+-------------+-----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('datasets/AA_DFW_2014_Departures_Short.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a23765-312e-4438-a5a9-c6e69bd9bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date (MM/DD/YYYY): string (nullable = true)\n",
      " |-- Flight Number: string (nullable = true)\n",
      " |-- Actual elapsed time (Minutes): string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa_dfw_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858da5c9-4dbc-48c8-916b-a04c82e6499a",
   "metadata": {},
   "source": [
    "## 3. Understanding Parquet\n",
    "\n",
    "### Diffculties with CSV files\n",
    "\n",
    "- No defined schema\n",
    "- Nested data requires special handling\n",
    "- Encoding format limited\n",
    "\n",
    "In specific with Spark, CSV files are:\n",
    "\n",
    "- Slow to be parsed.\n",
    "- Files cannot be filtered (no predicate pushdown), reduces the amount of information that must be processed in large datasets.\n",
    "- Any intermediate use required redefining schema.\n",
    "\n",
    "### The Parquet format\n",
    "\n",
    "Parquet is a compressed columnar data format developed for use in any Hadoop-based system (Spark, Hadoop, Apache Impala, etc.). The Parquet format is structured with data accessible in chunks, allowing efficient read/write operations without processing the entire file. This structured format supports Spark's predicate pushdown functionality, providing significant performance improvement. Finally, Parquet files automatically include schema information and handle data encoding. This is perfect for intermediary or on-disk representation of processed data. Not that Parquet files are a binary file format and can only be used with the proper tools.\n",
    "\n",
    "Reading a parquet file into a Dataframe can be used via `spark.read.format('parquet').load(<parquet_file>)` or `spark.read.parquet(<parquet_file>)`.\n",
    "\n",
    "Writing can be used using either `df.write.format('parquet').save(<parquet_file>)` or `df.write.parquet(<parquet_file>)`.\n",
    "\n",
    "The long-form versions of each permit extra option flags, such as when overwriting an existing parquet file.\n",
    "\n",
    "### Parquet and SQL\n",
    "\n",
    "Once a dataframe is retrieved, we can use the `.createOrReplaceTempView()` then use normal SQL syntax and the `spark.sql` method (as demonstrated in Course 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1ff06c-8210-49de-9738-4c884bd14e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157198\n"
     ]
    }
   ],
   "source": [
    "# View the row count of aa_dfw_df\n",
    "print(aa_dfw_df.count())\n",
    "\n",
    "# Save the DataFrame in Parquet format\n",
    "aa_dfw_df.write.format('parquet').save('datasets/example.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('datasets/example.parquet').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d726a-6403-46a1-bfd6-a264f8e5dab8",
   "metadata": {},
   "source": [
    "## 4. Dataframe operations\n",
    "\n",
    "### Filtering data\n",
    "\n",
    "Can be used within `df.filter()` or `df.where()`.\n",
    "\n",
    "Some example:\n",
    "- `voter_df.filter(voter_df['name'].isNotNull())`\n",
    "- `voter_df.filter(voter_df.date.year > 1800)`\n",
    "- `voter_df.filter(voter_df.name.like('M%'))`\n",
    "- `voter_df.where(voter_df['_c0'].contains('VOTE'))`\n",
    "- `voter_df.where(~ voter_df._c1.isNull())`\n",
    "\n",
    "### String transformation\n",
    "\n",
    "- `voter_df.withColumn('upper', F.upper('name'))`\n",
    "- `voter_df.withColumn('splits', F.split('name', ' '))`\n",
    "- `voter_df.withColumn('year', voter_df['_c4'].cast(IntegerType()))`\n",
    "\n",
    "### ArrayType() column functions\n",
    "\n",
    "- `.size(<column>)`\n",
    "- `.getItem(<index>)`\n",
    "\n",
    "### Conditional clauses\n",
    "\n",
    "- `df.select(df.Name, df.Age, F.when(df.Age >= 60, \"Elder\").when(df.Age >= 18, \"Adult\").otherwise(\"Minor\"))`\n",
    "\n",
    "### User defined function\n",
    "\n",
    "First, define a Python method:\n",
    "\n",
    "```python\n",
    "def reverseString(mystr):\n",
    "    return mystr[::-1]\n",
    "```\n",
    "\n",
    "Then, wrap the function and store as a variable\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "udfReverseString = udf(reverseString, StringType())\n",
    "```\n",
    "\n",
    "Finally, use with Spark:\n",
    "\n",
    "```python\n",
    "user_df = user_df.withColumn('ReverseName', udfReverseString(user_df.Name))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d408b8-d081-441a-a655-68f0e892892c",
   "metadata": {},
   "source": [
    "## 5. Caching\n",
    "\n",
    "### Caching in Spark\n",
    "\n",
    "Caching in Spark refers to storing the results of a Dataframe in memory or on disk of the processing nodes in a cluster.\n",
    "\n",
    "Caching improves the speed of subsequent transformations or actions as the data likely no longer needs to be retrieved from the original data source. Using caching reduces the resource utilization of the cluster - there is less need to access the storage, networking, and CPU of the Spark nodes as the data is likely already present.\n",
    "\n",
    "### Disadvantages of caching\n",
    "\n",
    "- Very large datasets may not fit in memory.\n",
    "- Local disk based caching may not be a performance improvement.\n",
    "- Cached objects may not be available.\n",
    "\n",
    "### Caching tips\n",
    "\n",
    "- Cache only if you need it\n",
    "- Try caching Dataframes at various points and determine if your performance improves\n",
    "- Cache in memory and fast SSD / NVMe storage\n",
    "- Cache to slow local disk if needed\n",
    "- Use intermediate files!\n",
    "- Stop caching objects when finished\n",
    "\n",
    "### Implementing caching\n",
    "\n",
    "Call `.cache()` on the Dataframe before action. It can be considered as a transformation, nothing is actually cached until an action is called.\n",
    "\n",
    "Use `.is_cached` boolean property which returns True if the Dataframe is cached.\n",
    "\n",
    "To un-cache a Dataframe, we call `.unpersist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c18c32-acbf-4e37-b10d-2077f19ca8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = spark.read.format('csv').options(Header=True).load('datasets/DallasCouncilVoters.csv.gz')\n",
    "df.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d8b620c-bb13-4c6e-b78a-be70046e2126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------------+---+\n",
      "|      DATE|        TITLE|         VOTER_NAME| ID|\n",
      "+----------+-------------+-------------------+---+\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates|  0|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston|  1|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings|  2|\n",
      "|02/08/2017|Councilmember|       Adam Medrano|  3|\n",
      "|02/08/2017|Councilmember|       Casey Thomas|  4|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold|  5|\n",
      "|02/08/2017|Councilmember|       Scott Griggs|  6|\n",
      "|02/08/2017|Councilmember|   B. Adam  McGough|  7|\n",
      "|02/08/2017|Councilmember|       Lee Kleinman|  8|\n",
      "|02/08/2017|Councilmember|      Sandy Greyson|  9|\n",
      "|02/08/2017|Councilmember|  Jennifer S. Gates| 10|\n",
      "|02/08/2017|Councilmember| Philip T. Kingston| 11|\n",
      "|02/08/2017|        Mayor|Michael S. Rawlings| 12|\n",
      "|02/08/2017|Councilmember|       Adam Medrano| 13|\n",
      "|02/08/2017|Councilmember|       Casey Thomas| 14|\n",
      "|02/08/2017|Councilmember|Carolyn King Arnold| 15|\n",
      "|02/08/2017|Councilmember| Rickey D. Callahan| 16|\n",
      "|01/11/2017|Councilmember|  Jennifer S. Gates| 17|\n",
      "|04/25/2018|Councilmember|     Sandy  Greyson| 18|\n",
      "|04/25/2018|Councilmember| Jennifer S.  Gates| 19|\n",
      "+----------+-------------+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('ID', F.monotonically_increasing_id())\n",
    "df = df.cache()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b09b90-bdfd-43e2-be87-7f76a2c64961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4f08671-0aec-45ac-b6d2-7452b73e4369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()\n",
    "df.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a156d7b-c577-4f54-8246-ce2b445914b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting 146558 rows took 1.149859 seconds\n",
      "Counting 146558 rows again took 0.306027 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create departures_df\n",
    "departures_df = spark.read.format('csv').options(Header=True).load('datasets/AA_DFW_2015_Departures_Short.csv.gz')\n",
    "\n",
    "# Add caching to the unique rows in departures_df\n",
    "departures_df = departures_df.distinct().cache()\n",
    "\n",
    "# Count the unique rows in departures_df, noting how long the operation takes\n",
    "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
    "\n",
    "# Count the rows again, noting the variance in time of a cached DataFrame\n",
    "start_time = time.time()\n",
    "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee9597-3051-4081-aea2-d40889fa7f48",
   "metadata": {},
   "source": [
    "## 6. Improve import performance\n",
    "\n",
    "### Spark clusters\n",
    "\n",
    "Spark Clusters consist of 2 types of processes - one driver process and as many worker processes as required. The driver handles task assignments and consolidation of the data results from the workers. The workers typically handle the actual transformation / action tasks of a Spark job. Once assigned tasks, they operate fairly independently and report results back to the driver. It is possible to have a single node Spark cluster but rare in a production environment.\n",
    "\n",
    "### Import performance\n",
    "\n",
    "More smaller objects better than larger ones with the same total amount. Spark can import via wildcard, for example:\n",
    "```python\n",
    "airport_df = spark.read.csv('airports-*.txt.gz')\n",
    "```\n",
    "\n",
    "We can split objects using OS utilities/scripts (split, cut, awk)\n",
    "```bash\n",
    "split -l 10000 -d largefile chunk-\n",
    "```\n",
    "or write out to Parquet\n",
    "```bash\n",
    "df_csv = spark.read.csv('singlelargefile.csv')\n",
    "df_csv.write.parquet('data.parquet')\n",
    "df = spark.read.parquet('data.parquet')\n",
    "```\n",
    "\n",
    "Best practice (?): Number of partitions/small files > Number of available nodes.\n",
    "\n",
    "Well-defined schemas in Spark drastically improve import performance. Without a schema defined, import tasks require reading the data multiple times to infer structure. This is very slow and Spark may not define the objects in the data as expected as well. Spark schemas also provide validation on import, which can save steps with data cleaning jobs and improve the overall processing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cac48e-20f5-4f4c-b596-ef266a8f28c2",
   "metadata": {},
   "source": [
    "## 7. Cluster configurations\n",
    "\n",
    "### Configuration options\n",
    "\n",
    "Spark has many available configuration settings controlling all aspects of the installation. These configurations can be modified to best match the specific needs for the cluster. The configurations are available in (1) the configuration files, (2) via the Spark web interface, and (3) via the run-time code.\n",
    "\n",
    "For the third option, to read a configuration setting, call `spark.conf.get()` with the name of the setting as the argument. To write a configuration setting, call `spark.conf.set()` with the name of the setting and the actual value as the function arguments.\n",
    "\n",
    "### Cluster Types\n",
    "\n",
    "- Single node clusters: deploy all components on a single system (physical / VM / container).\n",
    "- Standalone clusters: with dedicated machines as the driver and workers.\n",
    "- Managed clusters: meaning that the cluster components are handled by a third party cluster manager (YARN, Mesos, Kubernetes).\n",
    "\n",
    "### Driver\n",
    "\n",
    "There is one driver per Spark cluster. The driver is responsible for:\n",
    "- Handling task assignment for nodes/processes in the cluster.\n",
    "- Monitoring the state of all processes and tasks\n",
    "- Handling any task retries\n",
    "- Consolidating results from the other processes in the cluster\n",
    "- Handling any access to shared data and verifies each worker process has the necessary resources (code, data, etc.)\n",
    "\n",
    "It is often worth increasing the specifications of the node compared to other systems. Doubling the memory compared to other nodes is recommended. This is useful for task monitoring and data consolidation tasks. As with all Spark systems, fast local storage is useful for running Spark in an ideal setup.\n",
    "\n",
    "### Worker\n",
    "\n",
    "A Spark worker handles running tasks assigned by the driver and communicates those results back to the driver.\n",
    "\n",
    "Ideally, the worker has a copy of all code, data, and access to the necessary resources required to complete a given task. If any of these are unavailable, the worker must pause to obtain the resources.\n",
    "\n",
    "Recommendations:\n",
    "- Depending on the type of task, more worker nodes is often better than larger nodes.\n",
    "- Test various configurations to find the correct balance for your workload.\n",
    "- Workers can make use of fast local storage (SSD/NVMe) for caching, intermediate files, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d73d54-d9e7-4ecf-9822-dfc9c41420a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Spark\n",
      "Driver TCP port: 60519\n",
      "Number of partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f09e3-0ba4-4bb6-9c7d-7c2f0486b48c",
   "metadata": {},
   "source": [
    "## 8. Performance improvements\n",
    "\n",
    "### Spark execution plan\n",
    "\n",
    "To see the estimated plan that will be run to generate results from the DataFrame, we can use `.explain()`.\n",
    "\n",
    "### Shuffling\n",
    "\n",
    "Shuffling is the moving of data fragments to various workers as required to complete certain tasks. Shuffling is useful and hides overall complexity from the user. That being said, it can be slow to complete the necessary transfers, especially if a few nodes require all the data. Shuffling lowers the overall throughput of the cluster as the workers must spend time waiting for the data to transfer. This limits the amount of available workers for the remaining tasks in the system. Shuffling is often a necessary component, but it's helpful to try to minimize it as much as possible.\n",
    "\n",
    "### How to limit shuffling?\n",
    "\n",
    "Repartitioning by using `.repartition(num_partitions)` requires a full shuffle of data between nodes & processes and is quite costly.\n",
    "\n",
    "If you need to reduce the number of partitions, use the `.coalesce(num_partitions)` instead. It takes a number of partitions smaller than the current one and consolidates the data without requiring a full data shuffle. (Calling `.coalesce()` with a larger number of partitions does not actually do anything).\n",
    "\n",
    "Calling `.join()` indiscriminately can often cause shuffle operations, leading to increased cluster load & slower processing times. To avoid some of the shuffle operations whne joining Spark DataFrames, you can use the `.broadcast()` function.\n",
    "\n",
    "### Broadcasting\n",
    "\n",
    "Broadcasting in Spark is a method to provide a copy of an object to each worker. When each worker has its own copy of the data, there is less need for communication between nodes. This limits data shuffles and it's more likely a node will fulfil tasks independently.\n",
    "\n",
    "Using broadcasting can drastically speed up `.join()` operations, especially if one of the Dataframes being joined is much smaller than the other. Broadcasting can slow operations when using very small Dataframes or if you broadcast the larger Dataframe in a join. Spark will often optimize this for you, but as usual, run tests in your environment for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6befd8a-f10d-40c0-a8f4-6a5c0cd35397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [id#922], [id#964], Inner\n",
      "   :- Sort [id#922 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#922, 200), ENSURE_REQUIREMENTS, [plan_id=438]\n",
      "   :     +- Filter isnotnull(id#922)\n",
      "   :        +- FileScan json [created_at#919,description#920,entities#921,id#922,location#923,name#924,pinned_tweet_id#925L,profile_image_url#926,protected#927,public_metrics#928,url#929,username#930,verified#931,withheld#932] Batched: false, DataFilters: [isnotnull(id#922)], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/Users/mufin/Downloads/Twibot-22/user.json], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<created_at:string,description:string,entities:struct<description:struct<cashtags:array<str...\n",
      "   +- Sort [id#964 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#964, 200), ENSURE_REQUIREMENTS, [plan_id=439]\n",
      "         +- Filter isnotnull(id#964)\n",
      "            +- FileScan csv [id#964,label#965] Batched: false, DataFilters: [isnotnull(id#964)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/mufin/Downloads/Twibot-22/label.csv], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:string,label:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df = spark.read.json('user.json')\n",
    "label_df = spark.read.csv('label.csv', header=True)\n",
    "\n",
    "# Join the user_df and label_df\n",
    "full_df = user_df.join(label_df, user_df['id'] == label_df['id'])\n",
    "\n",
    "# Show the query plan\n",
    "full_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9139b4fc-36c3-4a95-a5a7-aa89f9cd1031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [id#922], [id#964], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(id#922)\n",
      "   :  +- FileScan json [created_at#919,description#920,entities#921,id#922,location#923,name#924,pinned_tweet_id#925L,profile_image_url#926,protected#927,public_metrics#928,url#929,username#930,verified#931,withheld#932] Batched: false, DataFilters: [isnotnull(id#922)], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/Users/mufin/Downloads/Twibot-22/user.json], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<created_at:string,description:string,entities:struct<description:struct<cashtags:array<str...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=464]\n",
      "      +- Filter isnotnull(id#964)\n",
      "         +- FileScan csv [id#964,label#965] Batched: false, DataFilters: [isnotnull(id#964)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/mufin/Downloads/Twibot-22/label.csv], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:string,label:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the DataFrames using broadcasting\n",
    "broadcast_df = user_df.join(broadcast(label_df), user_df['id'] == label_df['id'])\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b3d133a-6946-4788-8ce1-e6cbfa4b16c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal count:\t\t1000000\tduration: 5.483439\n",
      "Broadcast count:\t1000000\tduration: 4.245647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Count the number of rows in the normal DataFrame\n",
    "normal_count = full_df.count()\n",
    "normal_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "# Count the number of rows in the broadcast DataFrame\n",
    "broadcast_count = broadcast_df.count()\n",
    "broadcast_duration = time.time() - start_time\n",
    "\n",
    "# Print the counts and the duration of the tests\n",
    "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
    "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
